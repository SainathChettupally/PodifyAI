{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8ee3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: c:\\Users\\csain\\Downloads\\podifyai_deliverable 2\n",
      "src exists: True\n",
      "Python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "Torch: 2.9.0+cpu\n",
      "Transformers: 4.57.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\csain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch, transformers\n",
    "\n",
    "# Adjust path so we can import from src/\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"src exists:\", (repo_root / \"src\").exists())\n",
    "\n",
    "# Confirm key libraries\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "\n",
    "from src.summarizer import get_summarizer\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95844ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csain\\Downloads\\podifyai_deliverable 2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\csain\\.cache\\huggingface\\hub\\datasets--cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 287113/287113 [00:02<00:00, 131281.73 examples/s]\n",
      "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 130030.60 examples/s]\n",
      "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 136645.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 examples from CNN/DailyMail dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:20]\")\n",
    "print(f\"Loaded {len(dataset)} examples from CNN/DailyMail dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e3a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 320, but your input_length is only 235. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=117)\n",
      "Your max_length is set to 320, but your input_length is only 143. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=71)\n",
      "Your max_length is set to 320, but your input_length is only 141. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "summarizer_pipeline = get_summarizer()\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    article = example['article']\n",
    "    reference_summary = example['highlights']\n",
    "\n",
    "    # Generate summary using our pipeline\n",
    "    # Using 'standard' mode for a balanced summary length\n",
    "    generated_summary = summarizer_pipeline(\n",
    "        article,\n",
    "        max_length=320, # Corresponds to 'standard' mode target\n",
    "        min_length=max(30, 320 // 3),\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )[0]['summary_text'].strip()\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "    \n",
    "    results.append({\n",
    "        'example_id': i + 1,\n",
    "        'reference_summary': reference_summary,\n",
    "        'generated_summary': generated_summary,\n",
    "        'rouge1_fmeasure': scores['rouge1'].fmeasure,\n",
    "        'rouge2_fmeasure': scores['rouge2'].fmeasure,\n",
    "        'rougeL_fmeasure': scores['rougeL'].fmeasure,\n",
    "    })\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7498b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample of Results ---\n",
      "   example_id  rouge1_fmeasure  rouge2_fmeasure  rougeL_fmeasure\n",
      "0           1         0.406015         0.274809         0.360902\n",
      "1           2         0.403101         0.141732         0.310078\n",
      "2           3         0.381679         0.263566         0.259542\n",
      "3           4         0.312925         0.096552         0.231293\n",
      "4           5         0.320513         0.064935         0.179487\n",
      "\n",
      "--- Average ROUGE Scores ---\n",
      "rouge1_fmeasure    0.294454\n",
      "rouge2_fmeasure    0.126099\n",
      "rougeL_fmeasure    0.208447\n",
      "dtype: float64\n",
      "\n",
      "Full results saved to c:\\Users\\csain\\Downloads\\podifyai_deliverable 2\\results\\summarization_rouge_scores.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n--- Sample of Results ---\")\n",
    "print(df[['example_id', 'rouge1_fmeasure', 'rouge2_fmeasure', 'rougeL_fmeasure']].head())\n",
    "\n",
    "print(\"\\n--- Average ROUGE Scores ---\")\n",
    "print(df[['rouge1_fmeasure', 'rouge2_fmeasure', 'rougeL_fmeasure']].mean())\n",
    "\n",
    "# Optional: Save full results to a CSV\n",
    "output_csv_path = repo_root / \"results\" / \"summarization_rouge_scores.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nFull results saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f10d12",
   "metadata": {},
   "source": [
    "The ROUGE scores provide a quantitative measure of the quality of our summarization pipeline. Here\\'s a more detailed breakdown of the scores from our 20-sample evaluation:\n",
    "\n",
    "**Average Scores:**\n",
    "*   **ROUGE-1 F-measure (Unigram Overlap):** ~0.295\n",
    "*   **ROUGE-2 F-measure (Bigram Overlap):** ~0.126\n",
    "*   **ROUGE-L F-measure (Longest Common Subsequence):** ~0.208\n",
    "\n",
    "**Performance Range (Highest vs. Lowest Scores):**\n",
    "*   **ROUGE-1:**\n",
    "    *   **Highest:** 0.443 (Example 7)\n",
    "    *   **Lowest:** 0.162 (Example 6)\n",
    "*   **ROUGE-2:**\n",
    "    *   **Highest:** 0.275 (Example 1)\n",
    "    *   **Lowest:** 0.026 (Example 16)\n",
    "*   **ROUGE-L:**\n",
    "    *   **Highest:** 0.361 (Example 1)\n",
    "    *   **Lowest:** 0.104 (Example 14)\n",
    "\n",
    "### What do these scores mean?\n",
    "\n",
    "*   **General Performance:** The average scores are reasonable for an out-of-the-box, pre-trained model like DistilBART. They indicate that the model is generating summaries that have a moderate overlap with the human-written reference summaries.\n",
    "\n",
    "*   **Inconsistent Performance:** The wide range between the highest and lowest scores for all ROUGE metrics is a key finding. It suggests that the model\\'s performance is not consistent across all types of articles. For some articles (like example 7), it produces a summary with good keyword overlap (ROUGE-1 of 0.443), while for others (like example 6), it struggles.\n",
    "\n",
    "*   **ROUGE-1 vs. ROUGE-2:** The average ROUGE-1 score (~0.295) is significantly higher than the average ROUGE-2 score (~0.126). This is a common pattern and indicates that while the model is fairly good at capturing individual keywords, it is less successful at reproducing the exact two-word phrases found in the reference summaries. This points to the abstractive nature of the model, which rephrases content rather than just copying it.\n",
    "\n",
    "*   **ROUGE-L:** The average ROUGE-L score (~0.208), which falls between ROUGE-1 and ROUGE-2, suggests that the model can capture some of the main points and sentence structure of the reference summaries, but there is room for improvement in overall coherence.\n",
    "\n",
    "### Qualitative Observations\n",
    "\n",
    "By examining the generated summaries in the `summarization_rouge_scores.csv` file, we can make some qualitative observations:\n",
    "\n",
    "*   The summaries are generally fluent and readable.\n",
    "*   They successfully capture the main topic of the articles.\n",
    "*   However, they sometimes miss key details or contain minor factual inconsistencies, which is a common issue with abstractive summarization models.\n",
    "\n",
    "Overall, this evaluation demonstrates that the summarization pipeline is functioning and provides a good baseline for future improvements. The inconsistency in performance across different articles is an important finding to highlight in your report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6913e",
   "metadata": {},
   "source": [
    "## My Vision for the Next Iteration: Upgrading to Gemini\n",
    "\n",
    "This initial prototype has been a great success in establishing a functioning end-to-end pipeline using open-source models. The evaluation results provide a solid baseline. However, my vision for the next iteration of PodifyAI is to elevate its capabilities significantly by integrating the latest Gemini models via their API. This will be a major step forward, transitioning from a proof-of-concept to a truly state-of-the-art AI application.\n",
    "\n",
    "Here's my roadmap for leveraging Gemini:\n",
    "\n",
    "1.  **Leveraging Gemini for Advanced Summarization and Domain Adaptation:**\n",
    "    *   Instead of being limited to a general-purpose summarizer, we can use Gemini's advanced reasoning and language understanding capabilities to generate much higher-quality summaries. I expect to see significant improvements in coherence, accuracy, and the ability to capture nuanced information. Furthermore, Gemini's adaptability will allow us to provide expert-level summaries for specific domains (e.g., legal, medical, academic) without the need for manual fine-tuning, which is a major advantage.\n",
    "\n",
    "2.  **Integrating a Unified Multimodal Model with Gemini:**\n",
    "    *   A key benefit of Gemini is its native multimodality. My plan is to extend PodifyAI to handle documents that contain not just text, but also images, charts, and tables. Gemini can understand and incorporate information from these modalities into the summary, providing a much more comprehensive and valuable output for our users. This opens up exciting new possibilities for the types of documents we can support.\n",
    "\n",
    "3.  **Handling Long-Context Documents with Gemini:**\n",
    "    *   Our current chunking strategy is a workaround for the limited context window of the DistilBART model. The latest Gemini models have a much larger context window (up to 1 million tokens). By integrating Gemini, we can eliminate the need for complex and potentially error-prone chunking, allowing us to process very long documents in a single pass. This will lead to more coherent and contextually aware summaries for extensive texts.\n",
    "\n",
    "4.  **Advanced Evaluation and A/B Testing with Gemini:**\n",
    "    *   Once Gemini is integrated, I plan to conduct a thorough A/B testing evaluation. We will compare the summaries generated by our current DistilBART-based pipeline with those generated by Gemini. We can use the same ROUGE and BERTScore metrics, but more importantly, we will be able to demonstrate a significant leap in quality.\n",
    "\n",
    "5.  **Validating Gemini's Superiority with Human-in-the-Loop Evaluation:**\n",
    "    *   To complement the quantitative metrics, I will conduct a human-in-the-loop study to validate the superiority of the Gemini-powered summaries. We will ask users to compare the outputs from both versions of the pipeline and provide qualitative feedback. I am confident that this will confirm that the move to Gemini provides a demonstrably better user experience.\n",
    "\n",
    "This strategic shift to Gemini will not just be an incremental improvement; it will be a transformative step that will establish PodifyAI as a cutting-edge solution in the document summarization space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
