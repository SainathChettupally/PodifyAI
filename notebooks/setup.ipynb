{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4378088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\csain\\Downloads\\podifyai_deliverable1 (1)\n",
      "src exists: True\n",
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch: 2.8.0+cpu\n",
      "Transformers: 4.51.3\n"
     ]
    }
   ],
   "source": [
    "# --- Environment and path setup ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch, transformers\n",
    "\n",
    "# Adjust path so we can import from src/\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"Repo root:\", repo_root)\n",
    "print(\"src exists:\", (repo_root / \"src\").exists())\n",
    "\n",
    "# Confirm key libraries\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3ba996-027f-4211-8739-1eb64523fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data/: ['Clustering.pdf', 'Sample.pdf']\n"
     ]
    }
   ],
   "source": [
    "# --- Check data folder and create a sample file if none exists ---\n",
    "data_dir = repo_root / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Show current files\n",
    "existing_files = [p.name for p in data_dir.glob(\"*\")]\n",
    "print(\"Files in data/:\", existing_files)\n",
    "\n",
    "# Create a small text file if the folder is empty\n",
    "if not existing_files:\n",
    "    sample_path = data_dir / \"sample.txt\"\n",
    "    sample_path.write_text(\n",
    "        \"PodifyAI converts documents into summaries using NLP.\\n\"\n",
    "        \"This is a small text file for verifying extraction and summarization.\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"Created sample file:\", sample_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b72ccf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file: C:\\Users\\csain\\Downloads\\podifyai_deliverable1 (1)\\data\\Clustering.pdf exists: True\n",
      "Extraction successful. Characters extracted: 23633\n",
      "Clustering Dr. Christan Grant Dr. Laura Melissa Cruz Castro CAP5771 – Introduction to Data Science University of Florida Outline Basic Concepts of Cluster Analysis Partitioning Methods Hierarchical Methods Density methods Density-based and grid-based methods What is Cluster Analysis? • What is a cluster? • A cluster is a collection of data objects which are • Similar (or related) to one another within the same group (i.e., cluster) • Dissimilar (or unrelated) to the objects in other groups (i.e., clusters) • Cluster analysis (or clustering, data segmentation, …) • Given a set of data points, partition them into a set of groups (i.e., clusters) which are as similar as possible • Cluster analysis is unsupervised learning (i.e., no predefined classes) • This contrasts with classification (i.e\n"
     ]
    }
   ],
   "source": [
    "# --- Test text extraction from any supported format ---\n",
    "from src.extractors import detect_and_extract_text, clean_text, truncate_for_demo\n",
    "\n",
    "# Choose your test file \n",
    "TEST_PATH = data_dir / \"Clustering.pdf\"   # or \"sample.pdf\", \"demo.docx\", etc.\n",
    "print(\"Using file:\", TEST_PATH, \"exists:\", TEST_PATH.exists())\n",
    "\n",
    "try:\n",
    "    raw = detect_and_extract_text(str(TEST_PATH))\n",
    "    print(\"Extraction successful. Characters extracted:\", len(raw))\n",
    "except Exception as e:\n",
    "    print(\"Error during extraction:\", e)\n",
    "    raw = (\n",
    "        \"PodifyAI demo text. Upload any supported file (PDF, DOCX, PPTX, TXT, MD, HTML, CSV) \"\n",
    "        \"to extract text and summarize. \" * 5\n",
    "    )\n",
    "\n",
    "text = truncate_for_demo(clean_text(raw))\n",
    "print(text[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8c9b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 234, but your input_length is only 230. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=115)\n",
      "Your max_length is set to 585, but your input_length is only 177. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary (chars): 981\n",
      "\n",
      "Clustering is unsupervised learning (i.e., no predefined classes) A cluster is a collection of data objects which are similar (or related) to one another within the same group . A good clustering method will produce high quality clusters . Quality of clustering depends on the similarity measure used by the method, and the ability to discover some or all of the hidden patterns . Density-based and grid-based methods include high-dimensional clustering . High-dimensional clusters: Partitioning a database D of n objects into a set of k clusters, such that the sum of squared distances is minimized . Probabilistic and generative models: Modeling data from a generative process . Partitionsing method: Partitionsed method . High dimensional clustering: Partionsing method . The answer is typically highly subjective. The answer to clustering is often highly subjective . Clustersing is often used to find hidden patterns in a data set of objects with different types of clusters .\n"
     ]
    }
   ],
   "source": [
    "# --- Length-safe, longer summaries with tunable word targets ---\n",
    "from src.summarizer import get_summarizer\n",
    "import re\n",
    "\n",
    "sm = get_summarizer()\n",
    "tok = sm.tokenizer\n",
    "max_in = getattr(sm.model.config, \"max_position_embeddings\", 1024) - 16  # model input cap\n",
    "\n",
    "# === knobs you can tweak ===\n",
    "PER_CHUNK_TARGET_WORDS = 180     # make bigger for longer chunk summaries (e.g., 220–300)\n",
    "FINAL_TARGET_WORDS     = 450     # final merged summary length (e.g., 600–900)\n",
    "MIN_WORDS_FRACTION     = 0.35    # min_length = fraction of max_length\n",
    "SENT_JOIN              = \" \"     # how we join sentences\n",
    "# ===========================\n",
    "\n",
    "def words_to_tokens(words: int) -> int:\n",
    "    # heuristic: ~1 word ≈ 1.3 tokens for BART tokenizer\n",
    "    return max(32, int(words * 1.3))\n",
    "\n",
    "def _summarize_block(txt: str, target_words: int) -> str:\n",
    "    tgt_tok = words_to_tokens(target_words)\n",
    "    min_tok = max(30, int(tgt_tok * MIN_WORDS_FRACTION))\n",
    "    out = sm(\n",
    "        txt,\n",
    "        max_length=tgt_tok,\n",
    "        min_length=min_tok,\n",
    "        do_sample=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return out[0][\"summary_text\"].strip()\n",
    "\n",
    "def summarize_chunked_long(txt: str) -> str:\n",
    "    # If short enough, do a single pass with FINAL_TARGET_WORDS\n",
    "    if len(tok(txt)[\"input_ids\"]) <= max_in:\n",
    "        return _summarize_block(txt, FINAL_TARGET_WORDS)\n",
    "\n",
    "    # Sentence-based chunking to keep chunks near the model limit\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', txt)\n",
    "    parts, cur = [], \"\"\n",
    "    for s in sentences:\n",
    "        cand = (cur + SENT_JOIN + s).strip() if cur else s\n",
    "        if len(tok(cand)[\"input_ids\"]) <= max_in:\n",
    "            cur = cand\n",
    "        else:\n",
    "            if cur:\n",
    "                parts.append(cur)\n",
    "            cur = s\n",
    "    if cur:\n",
    "        parts.append(cur)\n",
    "\n",
    "    # Summarize each chunk with a reasonably long target\n",
    "    chunk_summaries = [_summarize_block(p, PER_CHUNK_TARGET_WORDS) for p in parts]\n",
    "\n",
    "    # Merge the chunk summaries and compress gently to a longer final target\n",
    "    merged = \" \".join(chunk_summaries)\n",
    "    final = _summarize_block(merged, FINAL_TARGET_WORDS)\n",
    "    return final\n",
    "\n",
    "summary = summarize_chunked_long(text)\n",
    "print(\"Generated summary (chars):\", len(summary))\n",
    "print()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "976e5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to: C:\\Users\\csain\\Downloads\\podifyai_deliverable1 (1)\\results\\sample_summary.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results_dir = repo_root / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = results_dir / \"sample_summary.txt\"\n",
    "output_path.write_text(summary, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Summary saved to:\", output_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
